{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f5ca9b-1dc2-43ad-ac47-f4bfb992e752",
   "metadata": {},
   "source": [
    "# Continue Batching\n",
    "\n",
    "Blog: [小冬瓜AIGC:手撕 Inference (1) : LLM推理服务永动机—— Continue Batching](https://zhuanlan.zhihu.com/p/1974105325897544853)\n",
    "\n",
    "git: [dhcode-cpp/easy-infer](https://github.com/dhcode-cpp/easy-infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c873bb-e85d-44a2-9fc6-901e8ae187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from typing import Dict, List, Set, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f6738-ded5-4247-b951-cf6963dba890",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40809611-a49f-4a7d-97a7-cd72962b5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ContinueBatchingEngineConfig:\n",
    "    max_batch_size = 4 # 64\n",
    "    max_seq_len = 32 # 512\n",
    "    max_prompt_len:int = 16 # 128\n",
    "    \n",
    "    # model & kv cache\n",
    "    num_layers = 3\n",
    "    dim = 16\n",
    "    num_heads = 2\n",
    "    head_dim = 8\n",
    "    vocab_size = 20\n",
    "    \n",
    "config = ContinueBatchingEngineConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde082c3-02eb-4eca-ab99-a2ffc8c71658",
   "metadata": {},
   "source": [
    "## Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4365d5-86b9-46e6-9415-5d5288997d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0768e415-20c4-413c-bd51-6bac6b43e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self, \n",
    "                 request_id:int, \n",
    "                 prompt: List[int], \n",
    "                 max_len: int = 2048):\n",
    "        self.request_id = request_id\n",
    "        self.prompt = prompt\n",
    "        self.generated_tokens = []\n",
    "        self.status = \"REQUEST_WAITING\"  # waiting, running, completed\n",
    "        self.current_length = len(prompt)\n",
    "        self.max_length = max_len\n",
    "        \n",
    "    def add_token(self, token: int):\n",
    "        \"\"\"添加生成的token到请求中\"\"\"\n",
    "        self.generated_tokens.append(token)\n",
    "        self.current_length += 1\n",
    "        if self.is_finished():\n",
    "            self.status = \"REQUEST_COMPLETED\"\n",
    "            print(f'finished: ID.{self.request_id}, new_len:{len(self.generated_tokens)}')\n",
    "    \n",
    "    def is_finished(self) -> bool:\n",
    "        \"\"\"检查请求是否完成（达到最大长度或生成了EOS）\"\"\"\n",
    "        return (self.current_length >= self.max_length or \n",
    "                (self.generated_tokens and self.generated_tokens[-1] == EOS_TOKEN))\n",
    "    \n",
    "    def get_full_sequence(self) -> List[int]:\n",
    "        \"\"\"获取完整的序列（prompt + 生成的tokens）\"\"\"\n",
    "        return self.prompt + self.generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9841f0-cf09-4d3c-a43f-862f4e5d020d",
   "metadata": {},
   "source": [
    "## RequestManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2054f3-09c3-439e-92a5-ec0c8f65ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import deque\n",
    "\n",
    "class RequestManager:\n",
    "    \"\"\"管理所有请求的调度和状态\"\"\"\n",
    "    \n",
    "    def __init__(self, max_batch_size: int):\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.requests = {}  # request_id -> Request\n",
    "        self.waiting_queue = deque()\n",
    "        self.running_requests = set()\n",
    "        self.next_request_id = 0\n",
    "        \n",
    "    def add_request(self, prompt: List[int], max_seq_len: int) -> int:\n",
    "        \"\"\"添加新请求，返回请求ID\"\"\"\n",
    "        request_id = self.next_request_id\n",
    "        self.next_request_id += 1\n",
    "        request = Request(request_id, prompt, max_seq_len)\n",
    "        self.requests[request_id] = request\n",
    "        self.waiting_queue.append(request_id)\n",
    "        return request_id\n",
    "    \n",
    "    def get_available_slots(self) -> int:\n",
    "        \"\"\"获取可用的批次空位数量\"\"\"\n",
    "        return self.max_batch_size - len(self.running_requests)\n",
    "    \n",
    "    def get_pending_requests(self, max_count: int) -> List[Tuple[int, List[int]]]:\n",
    "        \"\"\"获取等待处理的请求\"\"\"\n",
    "        available_slots = self.get_available_slots()\n",
    "        count = min(max_count, available_slots, len(self.waiting_queue))\n",
    "        \n",
    "        requests_to_process = []\n",
    "        for _ in range(count):\n",
    "            if not self.waiting_queue:\n",
    "                break\n",
    "            request_id = self.waiting_queue.popleft()\n",
    "            request = self.requests[request_id]\n",
    "            request.status = \"REQUEST_RUNNING\"\n",
    "            self.running_requests.add(request_id)\n",
    "            requests_to_process.append((request_id, request.prompt))\n",
    "        return requests_to_process\n",
    "    \n",
    "    def update_request(self, request_id: int, next_token: int):\n",
    "        \"\"\"更新请求状态\"\"\"\n",
    "        if request_id in self.requests:\n",
    "            request = self.requests[request_id]\n",
    "            request.add_token(next_token)\n",
    "            if request.is_finished():\n",
    "                self.running_requests.discard(request_id)\n",
    "    \n",
    "    def has_pending_requests(self) -> bool:\n",
    "        \"\"\"检查是否有未完成的请求\"\"\"\n",
    "        return len(self.waiting_queue) > 0 or len(self.running_requests) > 0\n",
    "        \n",
    "    def get_num_pending_requests(self) -> bool:\n",
    "        return len(self.waiting_queue)\n",
    "        \n",
    "    def get_num_running_requests(self) -> bool:\n",
    "        return len(self.running_requests)\n",
    "        \n",
    "    def get_running_request_ids(self) -> List[int]:\n",
    "        \"\"\"获取当前正在运行的请求ID\"\"\"\n",
    "        return list(self.running_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8acd89-2b55-4865-8de3-0fbc285b7ce8",
   "metadata": {},
   "source": [
    "## KVCacheManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23899340-b1b2-40d5-9d12-2ad020b7da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCacheManager:\n",
    "    \"\"\"管理Transformer的KV缓存\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        # 初始化KV缓存 [layer, batch, seq, head, dim]\n",
    "        \n",
    "        self.k_cache = torch.zeros(config.num_layers, config.max_batch_size, \n",
    "                                   config.max_seq_len, config.num_heads, config.head_dim)\n",
    "        self.v_cache = torch.zeros(config.num_layers, config.max_batch_size, \n",
    "                                   config.max_seq_len, config.num_heads, config.head_dim)\n",
    "        \n",
    "        self.sequence_lengths = torch.zeros(config.max_batch_size, dtype=torch.long)\n",
    "        self.request_to_slot = {}  # request_id -> slot_index\n",
    "        self.slot_to_request = {}  # slot_index -> request_id\n",
    "        self.free_slots = set(range(config.max_batch_size))\n",
    "        \n",
    "    def has_active_requests(self) -> bool:\n",
    "        \"\"\"检查是否有活跃的请求\"\"\"\n",
    "        return len(self.slot_to_request) > 0\n",
    "    \n",
    "    def has_available_slots(self) -> bool:\n",
    "        \"\"\"检查是否有可用的槽位\"\"\"\n",
    "        return len(self.free_slots) > 0\n",
    "        \n",
    "    def get_available_slots(self) -> bool:\n",
    "        \"\"\"检查是否有可用的槽位\"\"\"\n",
    "        return len(self.free_slots)\n",
    "    \n",
    "    def allocate_slots(self, request_ids: List[int]) -> List[int]:\n",
    "        \"\"\"为请求分配槽位\"\"\"\n",
    "        allocated_slots = []\n",
    "        for request_id in request_ids:\n",
    "            if not self.free_slots:\n",
    "                break\n",
    "            slot_id = self.free_slots.pop()\n",
    "            self.request_to_slot[request_id] = slot_id\n",
    "            self.slot_to_request[slot_id] = request_id\n",
    "            self.sequence_lengths[slot_id] = 0\n",
    "            allocated_slots.append(slot_id)\n",
    "        return allocated_slots\n",
    "    \n",
    "    def free_slot(self, request_id: int):\n",
    "        \"\"\"释放请求占用的槽位\"\"\"\n",
    "        if request_id in self.request_to_slot:\n",
    "            slot_id = self.request_to_slot[request_id]\n",
    "            del self.request_to_slot[request_id]\n",
    "            del self.slot_to_request[slot_id]\n",
    "            self.free_slots.add(slot_id)\n",
    "            # 清空该槽位的缓存\n",
    "            self.k_cache[:, slot_id, :, :, :] = 0\n",
    "            self.v_cache[:, slot_id, :, :, :] = 0\n",
    "\n",
    "    def update_slots(self, slot_ids, new_kv_cache):\n",
    "        for i, layer_kv_cache in enumerate(new_kv_cache):\n",
    "            bsz, seq_len, num_heads, head_dim = layer_kv_cache[0].shape\n",
    "            if seq_len == 1: # decoding mode\n",
    "                cur_len = self.sequence_lengths[slot_ids]\n",
    "                self.k_cache[i, slot_ids, cur_len, :, :]  = layer_kv_cache[0][:,0,:,:]\n",
    "                self.v_cache[i, slot_ids, cur_len, :, :]  = layer_kv_cache[1][:,0,:,:]\n",
    "            else: # Prefill mode\n",
    "                self.k_cache[i, slot_ids, :seq_len, :, :]  = layer_kv_cache[0]\n",
    "                self.v_cache[i, slot_ids, :seq_len, :, :]  = layer_kv_cache[1]\n",
    "                \n",
    "    \n",
    "    def update_after_step(self, new_tokens: torch.Tensor):\n",
    "        \"\"\"更新步进后的序列长度\"\"\"\n",
    "        active_slots = list(self.slot_to_request.keys())\n",
    "        self.sequence_lengths[active_slots] += 1\n",
    "    \n",
    "    def get_active_slots_info(self) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"获取活跃槽位的信息\"\"\"\n",
    "        active_slots = list(self.slot_to_request.keys())\n",
    "        request_ids = [self.slot_to_request[slot] for slot in active_slots]\n",
    "        return active_slots, request_ids\n",
    "    \n",
    "    def get_kv_cache_for_slots(self, slot_ids: List[int]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"获取指定槽位的KV缓存\"\"\"\n",
    "        return self.k_cache[:, slot_ids], self.v_cache[:, slot_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa1c08-f60d-4d33-b654-bb1bc61077e2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c974c115-db89-4dde-bada-455f812bfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.dim = config.dim\n",
    "        self.head_dim = config.head_dim\n",
    "        self.WQ = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.WK = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.WV = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.WO = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X, kvcache=None, current_length=None):\n",
    "        bsz, seq_len, _ = X.shape\n",
    "        Q, K, V= self.WQ(X), self.WK(X), self.WV(X)\n",
    "        Q=Q.reshape(bsz, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K=K.reshape(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "        V=V.reshape(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        if kvcache is None:\n",
    "            K_, V_ = K, V\n",
    "        else:\n",
    "            K_cache = kvcache[0]\n",
    "            V_cache = kvcache[1]\n",
    "            \n",
    "            # # left padding 拼接方式\n",
    "            # K_ = torch.cat((K_cache, K), dim = 1)\n",
    "            # V_ = torch.cat((V_cache, V), dim = 1)\n",
    "\n",
    "            # right padding 填充方式\n",
    "            cache_col = torch.zeros(bsz, 1, self.num_heads, self.head_dim)\n",
    "            K_ = torch.cat( (K_cache, cache_col.clone()), dim = 1 )\n",
    "            V_ = torch.cat( (V_cache, cache_col.clone()), dim = 1 )\n",
    "            K_[:, current_length, :, :] = K\n",
    "            V_[:, current_length, :, :] = V\n",
    "            \n",
    "\n",
    "        K_ = K_.transpose(1,2)\n",
    "        V_ = V_.transpose(1,2)\n",
    "\n",
    "        S = Q@K_.transpose(2,3)//math.sqrt(self.head_dim)\n",
    "        P = F.softmax(S, dim = -1)\n",
    "        Z = P@V_\n",
    "        Z = Z.transpose(1,2).reshape(bsz, seq_len, self.dim)\n",
    "        O = self.WO(Z)\n",
    "\n",
    "        # activate & shorcut\n",
    "        O_ = X + self.act(O)\n",
    "        \n",
    "        return O_, [K, V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e341a4-21bf-46c1-9776-16708092cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.lm_head = nn.Linear(config.dim, config.vocab_size)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [ DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kvcaches=None, current_length=None):\n",
    "        layer_kvcaches=[]\n",
    "        X = self.embd(x)\n",
    "        \n",
    "        for i, block in enumerate(self.decoder):\n",
    "            if kvcaches == None:\n",
    "                X, layer_kvcache = block(X, None, None)\n",
    "            else:\n",
    "                X, layer_kvcache = block(X, \n",
    "                                         kvcache=[kvcaches[0][i], kvcaches[1][i]], \n",
    "                                         current_length=current_length)\n",
    "                \n",
    "            layer_kvcaches.append(layer_kvcache)\n",
    "        logits = self.lm_head(X)\n",
    "        return logits, layer_kvcaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f33fa-4ccd-4525-8745-5e0a6574d9b5",
   "metadata": {},
   "source": [
    "## ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceff3801-97d5-4174-8c15-16443ccb5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    \"\"\"封装模型的前向传播\"\"\"\n",
    "    \n",
    "    def __init__(self, model, kv_cache_manager: KVCacheManager):\n",
    "        self.model = model\n",
    "        self.kv_cache_manager = kv_cache_manager\n",
    "    \n",
    "    def prefill_requests(self, requests: List[Tuple[int, List[int]]]) -> torch.Tensor:\n",
    "        \"\"\"预填充新请求\"\"\"\n",
    "        if not requests:\n",
    "            return torch.tensor([])\n",
    "            \n",
    "        # 分配槽位\n",
    "        request_ids = [req_id for req_id, _ in requests]\n",
    "        slot_ids = self.kv_cache_manager.allocate_slots(request_ids)\n",
    "        \n",
    "        # 准备输入 - 将不同长度的 prompt 填充到相同长度\n",
    "        prompts = [prompt for _, prompt in requests]\n",
    "        max_len = max(len(prompt) for prompt in prompts)\n",
    "        \n",
    "        input_ids = torch.zeros(len(prompts), max_len, dtype=torch.long)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            input_ids[i, :len(prompt)] = torch.tensor(prompt)\n",
    "        \n",
    "        # 执行预填充\n",
    "        with torch.no_grad():\n",
    "            logits, layer_kvcaches = self.model(input_ids,)\n",
    "        \n",
    "        # 更新KV缓存\n",
    "        self._update_kv_cache(slot_ids, layer_kvcaches)\n",
    "        \n",
    "        # 返回最后一个token的logits\n",
    "        return logits[:, -1, :].unsqueeze(1) # bsz, seq_len, vocab_size\n",
    "    \n",
    "    def decode_next_tokens(self, next_tokens: torch.Tensor, slot_ids: List[int], current_length) -> torch.Tensor:\n",
    "        \"\"\"解码下一个token\"\"\"\n",
    "        if len(slot_ids) == 0:\n",
    "            return torch.tensor([])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, layer_kvcaches = self.model(\n",
    "                next_tokens,\n",
    "                kvcaches = self.kv_cache_manager.get_kv_cache_for_slots(slot_ids),\n",
    "                current_length = current_length,\n",
    "            )\n",
    "        \n",
    "        # 更新KV缓存\n",
    "        self._update_kv_cache(slot_ids, layer_kvcaches)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _update_kv_cache(self, slot_ids: List[int], new_kv_cache):\n",
    "        \"\"\"更新KV缓存\"\"\"\n",
    "        self.kv_cache_manager.update_slots(slot_ids, new_kv_cache)\n",
    "        return\n",
    "        \n",
    "    def generate_next_tokens(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"从logits生成下一个token（贪婪采样）\"\"\"\n",
    "        if len(logits) == 0:\n",
    "            return torch.tensor([])\n",
    "        return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075abe12-8b63-4726-b88b-2d812fa38001",
   "metadata": {},
   "source": [
    "## ContinueBatchingEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65d2edf6-6ca8-42ca-933d-863d2ca7796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinueBatchingEngine:\n",
    "    \"\"\"连续批处理主引擎\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config):\n",
    "        self.kv_cache_manager = KVCacheManager(config)\n",
    "        self.model_wrapper = ModelWrapper(model, self.kv_cache_manager)\n",
    "        self.request_manager = RequestManager(config.max_batch_size,)\n",
    "    \n",
    "    def add_request(self, prompt: List[int], max_seq_len) -> int:\n",
    "        \"\"\"添加新请求\"\"\"\n",
    "        return self.request_manager.add_request(prompt, max_seq_len)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        step 函数先 decoding 再 prefill, 模拟多个 step, 假设每一步都新增一个请求\n",
    "        t1: prefill req1\n",
    "        t2: decoding req1, prefill req2\n",
    "        t3: decoding req1,2 prefill req3\n",
    "        \"\"\"\n",
    "        # 阶段1: 处理解码（已有请求）\n",
    "        if self.kv_cache_manager.has_active_requests():\n",
    "            active_slots, request_ids = self.kv_cache_manager.get_active_slots_info()\n",
    "\n",
    "            # 准备输入token (上一个 step 生成的token)\n",
    "            input_tokens = torch.tensor([\n",
    "                self.request_manager.requests[req_id].generated_tokens[-1]\n",
    "                for req_id in request_ids\n",
    "            ], dtype=torch.long)\n",
    "            current_length = torch.tensor([\n",
    "                self.request_manager.requests[req_id].current_length\n",
    "                for req_id in request_ids\n",
    "            ], dtype=torch.long)\n",
    "            input_tokens = input_tokens.unsqueeze(dim = 1)\n",
    "            \n",
    "            # 解码\n",
    "            decoding_logits = self.model_wrapper.decode_next_tokens(input_tokens, active_slots, current_length)\n",
    "            next_tokens = self.model_wrapper.generate_next_tokens(decoding_logits)\n",
    "            \n",
    "            # 更新状态\n",
    "            self.kv_cache_manager.update_after_step(next_tokens)\n",
    "            for i, request_id in enumerate(request_ids):\n",
    "                self.request_manager.update_request(request_id, next_tokens[i].item())\n",
    "                if self.request_manager.requests[request_id].is_finished():\n",
    "                    self.kv_cache_manager.free_slot(request_id)\n",
    "                    \n",
    "        # 阶段2: 处理预填充（新请求）\n",
    "        if self.kv_cache_manager.has_available_slots():\n",
    "            pending_requests = self.request_manager.get_pending_requests(\n",
    "                self.kv_cache_manager.get_available_slots()\n",
    "            )\n",
    "            if pending_requests:\n",
    "                prefill_logits = self.model_wrapper.prefill_requests(pending_requests)\n",
    "                prefill_tokens = self.model_wrapper.generate_next_tokens(prefill_logits)\n",
    "                for i, (request_id, _) in enumerate(pending_requests):\n",
    "                    self.request_manager.update_request(request_id, prefill_tokens[i].item())\n",
    "                    \n",
    "                    \n",
    "    def has_pending_work(self) -> bool:\n",
    "        \"\"\"检查是否还有未完成的工作\"\"\"\n",
    "        return self.request_manager.has_pending_requests()\n",
    "\n",
    "    def get_requests_info(self): \n",
    "        pending=self.request_manager.get_num_pending_requests()\n",
    "        running=self.request_manager.get_num_running_requests()\n",
    "        total_request=len(self.request_manager.requests)\n",
    "        return pending, running, total_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8badee-ecff-42fd-a1f4-2e2e3caddae7",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc89ac4-08f0-463d-ad89-003c1d0316af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def listen_request(config, p=0.01):\n",
    "    prompt=[]\n",
    "    prompt_len=0\n",
    "    num = randint(1,100)\n",
    "    if num/100.0 < p:\n",
    "        prompt_len = randint(config.max_prompt_len//4, config.max_prompt_len)\n",
    "        prompt=torch.randint(low=1, high=config.vocab_size, size=(1, prompt_len))\n",
    "        prompt=prompt[0].tolist()\n",
    "    return prompt, prompt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741ae609-af84-46fe-bc26-71320fde163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Request Info] pending:1/running:0/total:1\n",
      "[Request Info] pending:1/running:1/total:2\n",
      "Running...: * ---------\n",
      "[Request Info] pending:1/running:2/total:3\n",
      "[Request Info] pending:1/running:3/total:4\n",
      "[Request Info] pending:1/running:4/total:5\n",
      "Running...: ** --------\n",
      "[Request Info] pending:2/running:4/total:6\n",
      "[Request Info] pending:3/running:4/total:7\n",
      "[Request Info] pending:4/running:4/total:8\n",
      "Running...: *** -------\n",
      "[Request Info] pending:5/running:4/total:9\n",
      "[Request Info] pending:6/running:4/total:10\n",
      "[Request Info] pending:7/running:4/total:11\n",
      "finished: ID.0, new_len:17\n",
      "finished: ID.3, new_len:11\n",
      "Running...: **** ------\n",
      "[Request Info] pending:6/running:4/total:12\n",
      "finished: ID.2, new_len:14\n",
      "finished: ID.1, new_len:17\n",
      "finished: ID.6, new_len:2\n",
      "[Request Info] pending:4/running:4/total:13\n",
      "finished: ID.7, new_len:8\n",
      "[Request Info] pending:4/running:4/total:14\n",
      "finished: ID.4, new_len:11\n",
      "Running...: ***** -----\n",
      "[Request Info] pending:4/running:4/total:15\n",
      "[Request Info] pending:5/running:4/total:16\n",
      "finished: ID.8, new_len:12\n",
      "[Request Info] pending:5/running:4/total:17\n",
      "finished: ID.11, new_len:2\n",
      "finished: ID.12, new_len:1\n",
      "finished: ID.10, new_len:6\n",
      "finished: ID.12, new_len:2\n",
      "Running...: ****** ----\n",
      "[Request Info] pending:3/running:4/total:18\n",
      "[Request Info] pending:4/running:4/total:19\n",
      "finished: ID.5, new_len:18\n",
      "[Request Info] pending:4/running:4/total:20\n",
      "finished: ID.14, new_len:5\n",
      "finished: ID.15, new_len:3\n",
      "Running...: ******* ---\n",
      "[Request Info] pending:3/running:4/total:21\n",
      "finished: ID.16, new_len:3\n",
      "[Request Info] pending:3/running:4/total:22\n",
      "[Request Info] pending:4/running:4/total:23\n",
      "Running...: ******** --\n",
      "[Request Info] pending:5/running:4/total:24\n",
      "finished: ID.13, new_len:10\n",
      "finished: ID.17, new_len:7\n",
      "finished: ID.9, new_len:18\n",
      "[Request Info] pending:3/running:4/total:25\n",
      "finished: ID.21, new_len:4\n",
      "[Request Info] pending:3/running:4/total:26\n",
      "Running...: ********* -\n",
      "[Request Info] pending:4/running:4/total:27\n",
      "[Request Info] pending:5/running:4/total:28\n",
      "[Request Info] pending:6/running:4/total:29\n",
      "finished: ID.22, new_len:10\n",
      "finished: ID.19, new_len:16\n",
      "finished: ID.18, new_len:21\n",
      "Running...: ********** \n",
      "[Request Info] pending:4/running:4/total:30\n",
      "[Request Info] pending:5/running:4/total:31\n",
      "[Request Info] pending:6/running:4/total:32\n",
      "finished: ID.20, new_len:23\n",
      "finished: ID.24, new_len:10\n",
      "finished: ID.23, new_len:12\n",
      "finished: ID.28, new_len:2\n",
      "finished: ID.25, new_len:12\n",
      "finished: ID.30, new_len:7\n",
      "finished: ID.26, new_len:15\n",
      "finished: ID.29, new_len:13\n",
      "finished: ID.27, new_len:16\n",
      "finished: ID.31, new_len:19\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "N = 32\n",
    "count = 0\n",
    "\n",
    "model = ToyModel(config)\n",
    "engine = ContinueBatchingEngine(model, config)\n",
    "\n",
    "# main \n",
    "while 1:\n",
    "    # 监听进程\n",
    "    if count != N:\n",
    "        prompt, prompt_len = listen_request(config, p=0.5)\n",
    "        if prompt_len != 0:\n",
    "            count += 1\n",
    "            if count % (N//10) == 0:\n",
    "                per = count / (N//10) \n",
    "                print('Running...:','*'*int(per),'-'*(10-int(per)))\n",
    "            generate_len = randint(prompt_len, config.max_seq_len)\n",
    "            engine.add_request(prompt, generate_len)\n",
    "            pending, running, total = engine.get_requests_info()\n",
    "            print(f'[Request Info] pending:{pending}/running:{running}/total:{total}')\n",
    "            \n",
    "    # 处理进程   \n",
    "    engine.step()\n",
    "\n",
    "    if not engine.has_pending_work() and count == N:\n",
    "        print('process done')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
