{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e12fb8d-2144-4764-aa58-dce62d278bf2",
   "metadata": {},
   "source": [
    "# vLLM Page KVCache\n",
    "\n",
    "Blog: \n",
    "\n",
    "git: [dhcode-cpp/easy-infer](https://github.com/dhcode-cpp/easy-infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b91c59-0d38-456b-9bb7-5eadd1304630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1197c0e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77bfbe-2c4a-4498-8858-325b3d7b4247",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287c6984-a72f-40ce-9bec-3441815fff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class vLLMEngineConfig:\n",
    "    max_batch_size = 4\n",
    "    max_seq_len = 32\n",
    "    max_prompt_len: int = 16\n",
    "\n",
    "    # model & kv cache\n",
    "    num_layers = 3\n",
    "\n",
    "    # PageKV Cache Setting\n",
    "    page_size = 64\n",
    "    num_pages = 1024\n",
    "\n",
    "    dim = 16\n",
    "    num_heads = 2\n",
    "    head_dim = 8\n",
    "    vocab_size = 20\n",
    "\n",
    "config = vLLMEngineConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407940ef-f455-4541-b1d2-0925aa792149",
   "metadata": {},
   "source": [
    "# Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769481bf-761c-4103-9dda-2e8b6d126316",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 0\n",
    "class Request:\n",
    "    def __init__(self,\n",
    "                 request_id: int,\n",
    "                 prompt: List[int],\n",
    "                 max_len: int = 2048):\n",
    "        self.request_id = request_id\n",
    "        self.prompt = prompt\n",
    "        self.generated_tokens = []\n",
    "        self.status = \"REQUEST_WAITING\"  # WAITING, RUNNING, COMPLETED\n",
    "        self.current_length = len(prompt)\n",
    "        self.max_length = max_len\n",
    "\n",
    "    def add_token(self, token: int):\n",
    "        \"\"\"添加生成的token到请求中\"\"\"\n",
    "        self.generated_tokens.append(token)\n",
    "        self.current_length += 1\n",
    "        if self.is_finished():\n",
    "            self.status = \"REQUEST_COMPLETED\"\n",
    "            print(\n",
    "                f'finished: ID.{self.request_id}, new_len:{len(self.generated_tokens)}')\n",
    "\n",
    "    def is_finished(self) -> bool:\n",
    "        \"\"\"检查请求是否完成(达到最大长度或生成了EOS)\"\"\"\n",
    "        result = (self.current_length >= self.max_length or (\n",
    "            self.generated_tokens and self.generated_tokens[-1] == EOS_TOKEN))\n",
    "        return result\n",
    "\n",
    "    def get_full_sequence(self) -> List[int]:\n",
    "        \"\"\"获取完整的序列(prompt + 生成的tokens)\"\"\"\n",
    "        return self.prompt + self.generated_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c4c90-5719-4936-8792-970483ce57c6",
   "metadata": {},
   "source": [
    "## Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f0d0a0-73ad-4440-8f3f-6a42396a89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import deque\n",
    "\n",
    "class Schedular:\n",
    "    \"\"\"管理所有请求的调度和状态\"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len: int = 1024):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.requests = {}  # request_id -> Request\n",
    "        self.waiting_queue = deque()\n",
    "        self.running_requests = set()\n",
    "\n",
    "    def add_request(self, prompt: List[int], max_seq_len: int) -> int:\n",
    "        \"\"\"添加新请求，返回请求ID\"\"\"\n",
    "        request_id = len(self.requests)\n",
    "        request = Request(request_id, prompt, max_seq_len)\n",
    "        self.requests[request_id] = request\n",
    "        self.waiting_queue.append(request_id)\n",
    "        return request_id\n",
    "\n",
    "    def get_available_request(self) -> int:\n",
    "        \"\"\"获取可用的批次空位数量\"\"\"\n",
    "        return len(self.waiting_queue)\n",
    "\n",
    "    def get_pending_requests(self, max_count: int) -> List[Tuple[int, List[int]]]:\n",
    "        \"\"\"获取等待处理的请求\"\"\"\n",
    "        available_slots = self.get_available_request()\n",
    "        count = min(max_count, available_slots, len(self.waiting_queue))\n",
    "\n",
    "        requests_to_process = []\n",
    "        for _ in range(count):\n",
    "            if not self.waiting_queue:\n",
    "                break\n",
    "            request_id = self.waiting_queue.popleft()\n",
    "            request = self.requests[request_id]\n",
    "            request.status = \"REQUEST_RUNNING\"\n",
    "            self.running_requests.add(request_id)\n",
    "            requests_to_process.append((request_id, request.prompt))\n",
    "        return requests_to_process\n",
    "\n",
    "    def update_request(self, request_id: int, next_token: int):\n",
    "        \"\"\"更新请求状态\"\"\"\n",
    "        if request_id in self.requests:\n",
    "            request = self.requests[request_id]\n",
    "            request.add_token(next_token)\n",
    "            if request.is_finished():\n",
    "                self.running_requests.discard(request_id)\n",
    "\n",
    "    def has_pending_requests(self) -> bool:\n",
    "        \"\"\"检查是否有未完成的请求\"\"\"\n",
    "        return len(self.waiting_queue) > 0 or len(self.running_requests) > 0\n",
    "\n",
    "    def get_num_pending_requests(self) -> int:\n",
    "        return len(self.waiting_queue)\n",
    "\n",
    "    def get_num_running_requests(self) -> int:\n",
    "        return len(self.running_requests)\n",
    "\n",
    "    def get_running_request_ids(self) -> List[int]:\n",
    "        \"\"\"获取当前正在运行的请求ID\"\"\"\n",
    "        return list(self.running_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e95f-51ad-4d2c-82a3-d876c5c78770",
   "metadata": {},
   "source": [
    "## PageKVCache \n",
    "\n",
    "1. BlockTable\n",
    "2. PageKVCacheEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41edf32-927a-4184-994e-decc75f4227c",
   "metadata": {},
   "source": [
    "## BlockTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc2c026-2575-468b-9971-757cb506d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockTable:\n",
    "    \"\"\"逻辑块表管理 - 仅负责分页资源管理\"\"\"\n",
    "\n",
    "    def __init__(self, page_size: int, num_pages: int):\n",
    "        self.page_size = page_size\n",
    "        self.num_pages = num_pages\n",
    "        self.free_pages = list(range(num_pages))\n",
    "        self.allocated_pages = set()\n",
    "\n",
    "        self.page_usage = [0] * num_pages  \n",
    "        self.next_page = [-1] * num_pages  \n",
    "\n",
    "    def _allocate_pages(self, num_pages: int, parent_block_id=-1):\n",
    "        \"\"\"分配指定数量的页\"\"\"\n",
    "        if len(self.free_pages) < num_pages:\n",
    "            return None\n",
    "\n",
    "        allocated = self.free_pages[:num_pages]\n",
    "        self.free_pages = self.free_pages[num_pages:]\n",
    "        self.allocated_pages.update(allocated)\n",
    "\n",
    "        # 初始化块状态\n",
    "        for page_id in allocated:\n",
    "            self.page_usage[page_id] = 0\n",
    "            self.next_page[page_id] = -1\n",
    "\n",
    "        if parent_block_id != -1:\n",
    "            self.next_page[parent_block_id] = allocated[0]\n",
    "\n",
    "        return allocated\n",
    "\n",
    "    def _free_pages(self, page_ids: list[int]):\n",
    "        \"\"\"释放页\"\"\"\n",
    "        for page_id in page_ids:\n",
    "            if page_id in self.allocated_pages:\n",
    "                self.allocated_pages.remove(page_id)\n",
    "                self.free_pages.append(page_id)\n",
    "                self.page_usage[page_id] = 0\n",
    "                self.next_page[page_id] = -1\n",
    "\n",
    "    def get_free_count(self) -> int:\n",
    "        \"\"\"获取空闲块数量\"\"\"\n",
    "        return len(self.free_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c14f49-3c04-44a5-be4f-3ca054366a05",
   "metadata": {},
   "source": [
    "## PageKVCacheEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005f7f40-3771-4ba6-8785-2f78b7046fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageKVCacheEngine:\n",
    "    \"\"\"分页式管理 KV 缓存\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # 初始化KV缓存 [layer, batch, seq, head, dim]\n",
    "\n",
    "        self.num_pages = config.num_pages\n",
    "        self.page_size = config.page_size\n",
    "\n",
    "        self.block_table = BlockTable(self.page_size,\n",
    "                                      self.num_pages,)\n",
    "\n",
    "        # KV 的存储表由块表大小来管理\n",
    "        self.k_cache = torch.zeros(config.num_layers,\n",
    "                                   self.num_pages,\n",
    "                                   self.page_size,\n",
    "                                   config.num_heads,\n",
    "                                   config.head_dim)\n",
    "        self.v_cache = torch.zeros_like(self.k_cache)\n",
    "\n",
    "        # 每个请求的长度\n",
    "        self.sequence_lengths = {}\n",
    "\n",
    "        # 请求与 block_id 的映射信息\n",
    "        self.request_to_pages = {}  # request_id -> [page_id1, page_id2, ...]\n",
    "        self.page_to_request = {}  # page_id -> request_id\n",
    "\n",
    "    def has_active_requests(self) -> bool:\n",
    "        \"\"\"检查是否有活跃的请求\"\"\"\n",
    "        return len(self.request_to_pages) > 0\n",
    "\n",
    "    def has_available_pages(self, request_length) -> bool:\n",
    "        \"\"\"检查是否有可用的分页\"\"\"\n",
    "        free_num_pages = self.block_table.get_free_count()\n",
    "        return request_length < free_num_pages * self.page_size\n",
    "\n",
    "    def allocate_request_pages(self, request_id, request_length) -> List[int]:\n",
    "        \"\"\"\n",
    "        为 prefill 请求预分配页面, 对于正在解码的 decoding 请求，不需要预分配 page, 后续分配功能写在一起\n",
    "        \"\"\"\n",
    "\n",
    "        allocate_pages_size = (request_length // self.page_size)+1\n",
    "        allocate_pages_ids = self.block_table._allocate_pages(\n",
    "            allocate_pages_size)\n",
    "        if allocate_pages_ids == None:\n",
    "            print(f'[ALLOCATE] request ID{request_id} pages faild')\n",
    "            return []\n",
    "\n",
    "        self.request_to_pages[request_id] = allocate_pages_ids\n",
    "\n",
    "        for i in self.request_to_pages[request_id]:\n",
    "            self.page_to_request[i] = request_id\n",
    "        self.sequence_lengths[request_id] = 0\n",
    "\n",
    "        print(\n",
    "            f'[ALLOCATE] request ID{request_id} pages len {len(allocate_pages_ids)}')\n",
    "\n",
    "        return allocate_pages_ids\n",
    "\n",
    "    def free_request_pages(self, request_id: int):\n",
    "        \"\"\"释放请求占用的页面\"\"\"\n",
    "        allocate_pages_ids = self.request_to_pages[request_id]\n",
    "        self.block_table._free_pages([0, 1])\n",
    "\n",
    "        self.k_cache[:, allocate_pages_ids, :, :, :] = 0\n",
    "        self.v_cache[:, allocate_pages_ids, :, :, :] = 0\n",
    "\n",
    "        del self.request_to_pages[request_id]\n",
    "        for idx in allocate_pages_ids:\n",
    "            del self.page_to_request[idx]\n",
    "        print(\n",
    "            f\"[FREE] request ID{request_id} pages, len{len(allocate_pages_ids)}\")\n",
    "\n",
    "    def update_pages(self, request_id, new_kv_cache):\n",
    "        # 1. Prefill: 填充到 requst_id -> pages 上\n",
    "        # 2. Decoding: 找到 requst_id -> pages 上的最后一个 token, 如果最后一个块已满，需要重新申请一个新的块表。\n",
    "\n",
    "        # 1. Update Decoding blocks\n",
    "        # 对于 Decoding 存在增加一个新 token 导致要新加一个 block 的情况\n",
    "        seq_len, _, _ = new_kv_cache[0][0].shape  # 0 层数据 K数据\n",
    "        T = self.page_size\n",
    "\n",
    "        if seq_len == 1:\n",
    "            length = self.sequence_lengths[request_id]\n",
    "            pages_ids = self.request_to_pages[request_id]\n",
    "            if length % T == 0:\n",
    "                new_block_id = self.block_table._allocate_pages(\n",
    "                    1, pages_ids[-1])\n",
    "                self.request_to_pages[request_id].append(new_block_id)\n",
    "                self.page_to_request[new_block_id] = request_id\n",
    "            self.sequence_lengths[request_id] += 1\n",
    "            cur_offset_len = self.sequence_lengths[request_id] % T\n",
    "        else:\n",
    "            # prefill 填充 context 长度\n",
    "            self.sequence_lengths[request_id] = seq_len\n",
    "\n",
    "        # 2. Update Decoding Stage KV-Cache\n",
    "        if seq_len == 1:\n",
    "            for i, layer_kv_cache in enumerate(new_kv_cache):\n",
    "                # seq_len, num_heads, head_dim = layer_kv_cache[0].shape\n",
    "                pages_ids = self.request_to_pages[request_id]\n",
    "                cur_offset_len = self.sequence_lengths[request_id] % T\n",
    "\n",
    "                # 最后一个块上加 Cache\n",
    "                self.k_cache[i, pages_ids[-1], cur_offset_len,\n",
    "                             :, :] = layer_kv_cache[0][0, :, :]\n",
    "                self.v_cache[i, pages_ids[-1], cur_offset_len,\n",
    "                             :, :] = layer_kv_cache[1][0, :, :]\n",
    "\n",
    "        # 3. Update Prefill Stage KV-Cache\n",
    "        else:\n",
    "            for i, layer_kv_cache in enumerate(new_kv_cache):\n",
    "                pages_ids = self.request_to_pages[request_id]\n",
    "                for k, idx in enumerate(pages_ids):\n",
    "                    if k != len(pages_ids)-1:\n",
    "                        self.k_cache[i, idx, :, :,\n",
    "                                     :] = layer_kv_cache[0][k*T: (k+1)*T]\n",
    "                        self.v_cache[i, idx, :, :,\n",
    "                                     :] = layer_kv_cache[1][k*T: (k+1)*T]\n",
    "                    else:  # 最后一个 block\n",
    "                        cur_len = self.sequence_lengths[request_id] % T\n",
    "\n",
    "                        self.k_cache[i, idx, :cur_len, :,\n",
    "                                     :] = layer_kv_cache[0][k*T: k*T+cur_len]\n",
    "                        self.v_cache[i, idx, :cur_len, :,\n",
    "                                     :] = layer_kv_cache[1][k*T: k*T+cur_len]\n",
    "\n",
    "    def get_sequence_kvcache(self, request_ids: List[int]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"获取请求列表的batch KVCache 数据\"\"\"\n",
    "        T = self.page_size\n",
    "\n",
    "        max_pages = -1\n",
    "        for idx in request_ids:\n",
    "            max_pages = max(len(self.request_to_pages[idx]), max_pages)\n",
    "\n",
    "        num_layers, _, _, H, D = self.k_cache.shape\n",
    "\n",
    "        K = torch.zeros(num_layers, len(request_ids), max_pages * T, H, D)\n",
    "        V = torch.zeros(num_layers, len(request_ids), max_pages * T, H, D)\n",
    "\n",
    "        for t, idx in enumerate(request_ids):\n",
    "            page_ids = self.request_to_pages[idx]\n",
    "\n",
    "            cur_length = len(page_ids) * T\n",
    "\n",
    "            K[:, t, :cur_length, :, :] = self.k_cache[:, page_ids,\n",
    "                                                      :, :, :].reshape(num_layers, cur_length, H, D)\n",
    "            V[:, t, :cur_length, :, :] = self.v_cache[:, page_ids,\n",
    "                                                      :, :, :].reshape(num_layers, cur_length, H, D)\n",
    "\n",
    "        return (K, V)\n",
    "\n",
    "    def get_request_info(self, ):\n",
    "\n",
    "        for request_id, page_ids in self.request_to_pages.items():\n",
    "            print(f'----Req.ID:{request_id}----')\n",
    "            print('pages list:', page_ids)\n",
    "            print('cur_length:', self.sequence_lengths[request_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854bcab-4807-4f60-a287-2c107c229aa3",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Standard Attention, next-part will implemented PageAttention Kernel with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f6520c-a38a-4c48-9e37-7492b6b39492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.dim = config.dim\n",
    "        self.head_dim = config.head_dim\n",
    "        self.WQ = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.WK = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.WV = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.WO = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, X, kvcache=None, current_length=None):\n",
    "        bsz, seq_len, _ = X.shape\n",
    "        Q, K, V = self.WQ(X), self.WK(X), self.WV(X)\n",
    "        Q = Q.reshape(bsz, seq_len, self.num_heads,\n",
    "                      self.head_dim).transpose(1, 2)\n",
    "        K = K.reshape(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.reshape(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        if kvcache is None:\n",
    "            K_, V_ = K, V\n",
    "        else:\n",
    "            K_cache = kvcache[0]\n",
    "            V_cache = kvcache[1]\n",
    "\n",
    "            # # left padding 拼接方式\n",
    "            # K_ = torch.cat((K_cache, K), dim = 1)\n",
    "            # V_ = torch.cat((V_cache, V), dim = 1)\n",
    "\n",
    "            # right padding 填充方式\n",
    "            cache_col = torch.zeros(bsz, 1, self.num_heads, self.head_dim)\n",
    "            K_ = torch.cat((K_cache, cache_col.clone()), dim=1)\n",
    "            V_ = torch.cat((V_cache, cache_col.clone()), dim=1)\n",
    "            K_[:, current_length, :, :] = K\n",
    "            V_[:, current_length, :, :] = V\n",
    "\n",
    "        K_ = K_.transpose(1, 2)\n",
    "        V_ = V_.transpose(1, 2)\n",
    "\n",
    "        S = Q@K_.transpose(2, 3)//math.sqrt(self.head_dim)\n",
    "        P = F.softmax(S, dim=-1)\n",
    "        Z = P@V_\n",
    "        Z = Z.transpose(1, 2).reshape(bsz, seq_len, self.dim)\n",
    "        O = self.WO(Z)\n",
    "\n",
    "        # activate & shorcut\n",
    "        O_ = X + self.act(O)\n",
    "\n",
    "        return O_, (K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2aa2f6-fd08-47f3-a136-4bc82f467eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.lm_head = nn.Linear(config.dim, config.vocab_size)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kvcaches=None, current_length=None):\n",
    "        layer_kvcaches = []\n",
    "        X = self.embd(x)\n",
    "\n",
    "        for i, block in enumerate(self.decoder):\n",
    "            if kvcaches == None:\n",
    "                X, layer_kvcache = block(X, None, None)\n",
    "            else:\n",
    "                X, layer_kvcache = block(X,\n",
    "                                         kvcache=[kvcaches[0][i],\n",
    "                                                  kvcaches[1][i]],\n",
    "                                         current_length=current_length)\n",
    "\n",
    "            layer_kvcaches.append(layer_kvcache)\n",
    "        logits = self.lm_head(X)\n",
    "        return logits, layer_kvcaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211bd93-6119-4aea-9e1a-b9cd7c472ff5",
   "metadata": {},
   "source": [
    "## ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e5d3d8-9d66-4f1c-93e9-97e017a903c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelWrapper:\n",
    "    \"\"\"封装模型的前向传播\"\"\"\n",
    "\n",
    "    def __init__(self, model, kv_cache_manager: PageKVCacheEngine):\n",
    "        self.model = model\n",
    "        self.cacher = kv_cache_manager\n",
    "\n",
    "    def prefill_requests(self, request_ids: List[int], prompts: List[List[int]]) -> Tuple[torch.Tensor, Any]:\n",
    "        \"\"\"预填充新请求\"\"\"\n",
    "        if len(request_ids) == 0:\n",
    "            return torch.tensor([])\n",
    "\n",
    "        # 预分配页面\n",
    "        for request_id, prompt in zip(request_ids, prompts):\n",
    "            self.cacher.allocate_request_pages(request_id,  len(prompt))\n",
    "\n",
    "        # padding\n",
    "        current_lens = [len(prompt) for prompt in prompts]\n",
    "        max_len = max(current_lens)\n",
    "        input_ids = torch.zeros(len(prompts), max_len, dtype=torch.long)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            input_ids[i, :len(prompt)] = torch.tensor(prompt)\n",
    "\n",
    "        # 执行预填充\n",
    "        with torch.no_grad():\n",
    "            logits, layer_kvcaches = self.model(input_ids,\n",
    "                                                # current_length=current_len\n",
    "                                                )\n",
    "        L = [ l-1 for l in current_lens]\n",
    "        return logits[:, L, :], layer_kvcaches\n",
    "\n",
    "    def decode_next_tokens(self, next_tokens: torch.Tensor, current_length=None, KVCache=None) -> Tuple[torch.Tensor, Any]:\n",
    "        \"\"\"解码下一个token\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, layer_kvcaches = self.model(\n",
    "                next_tokens,\n",
    "                kvcaches=KVCache,\n",
    "                current_length=current_length,\n",
    "            )\n",
    "\n",
    "        return logits, layer_kvcaches\n",
    "\n",
    "    def generate_next_tokens(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"从logits生成下一个token(贪婪采样)\"\"\"\n",
    "        if len(logits) == 0:\n",
    "            return torch.tensor([])\n",
    "        return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0a1cc-586c-4a10-836e-b365683b1f72",
   "metadata": {},
   "source": [
    "## vLLM 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968ae34d-2cae-4c60-a9fa-ab084a02b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMPageCacheEngine:\n",
    "    \"\"\"vLLM 主引擎\"\"\"\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        self.cacher = PageKVCacheEngine(config)\n",
    "        self.model_wrapper = ModelWrapper(model, self.cacher)\n",
    "        self.schedular = Schedular(config.max_seq_len,)\n",
    "\n",
    "    def add_request(self, prompt: List[int], max_seq_len) -> int:\n",
    "        \"\"\"添加新请求\"\"\"\n",
    "        return self.schedular.add_request(prompt, max_seq_len)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        request_ids = None\n",
    "        layer_kvcaches = None\n",
    "        # 阶段1: 处理解码(已有请求)\n",
    "        if self.schedular.get_num_running_requests() > 0:\n",
    "            \n",
    "            request_ids = self.schedular.get_running_request_ids()\n",
    "\n",
    "\n",
    "            # 准备输入token (上一个 step 生成的token)\n",
    "            input_tokens = torch.tensor([\n",
    "                self.schedular.requests[req_id].generated_tokens[-1]\n",
    "                for req_id in request_ids\n",
    "            ], dtype=torch.long)\n",
    "            current_length = torch.tensor([\n",
    "                self.schedular.requests[req_id].current_length\n",
    "                for req_id in request_ids\n",
    "            ], dtype=torch.long)\n",
    "            input_tokens = input_tokens.unsqueeze(dim=1)\n",
    "\n",
    "            # Page KVCache -> Batch KVCache\n",
    "            batch_kvcache = self.cacher.get_sequence_kvcache(request_ids)\n",
    "\n",
    "            # 解码\n",
    "            logits, layer_kvcaches = self.model_wrapper.decode_next_tokens(input_tokens,\n",
    "                                                                           KVCache=batch_kvcache,\n",
    "                                                                           current_length=current_length)\n",
    "            next_tokens = self.model_wrapper.generate_next_tokens(logits)\n",
    "\n",
    "            # update kv cache\n",
    "            self.update_kvcache(request_ids, layer_kvcaches)\n",
    "\n",
    "            # 更新状态\n",
    "            for i, request_id in enumerate(request_ids):\n",
    "                self.schedular.update_request(\n",
    "                    request_id, next_tokens[i].item())\n",
    "                if self.schedular.requests[request_id].is_finished():\n",
    "                    self.cacher.free_request_pages(request_id)\n",
    "\n",
    "        # 阶段2: 处理预填充(新请求)\n",
    "        if self.schedular.get_num_pending_requests() > 0:\n",
    "            pending_requests = self.schedular.get_pending_requests(\n",
    "                config.num_pages\n",
    "            )\n",
    "            request_ids = [idx for idx, _ in pending_requests]\n",
    "            prompts = [prompt for _, prompt in pending_requests]\n",
    "\n",
    "            if pending_requests:\n",
    "                logits, layer_kvcaches = self.model_wrapper.prefill_requests(\n",
    "                    request_ids, prompts)\n",
    "                next_tokens = self.model_wrapper.generate_next_tokens(logits)\n",
    "                \n",
    "                for i, (request_id, _) in enumerate(pending_requests):\n",
    "                    self.schedular.update_request(\n",
    "                        request_id, next_tokens[0,i].item())\n",
    "\n",
    "                self.update_kvcache(request_ids, layer_kvcaches)\n",
    "\n",
    "    def update_kvcache(self, request_ids, layer_kvcaches):\n",
    "        if request_ids != None:\n",
    "            for i, idx in enumerate(request_ids):\n",
    "                tmp_cache = [[layer_kvcache[0][i], layer_kvcache[1][i]]\n",
    "                             for layer_kvcache in layer_kvcaches]\n",
    "                self.cacher.update_pages(idx, tmp_cache)\n",
    "\n",
    "    def has_pending_work(self) -> bool:\n",
    "        \"\"\"检查是否还有未完成的工作\"\"\"\n",
    "        return self.schedular.has_pending_requests()\n",
    "\n",
    "    def get_requests_info(self):\n",
    "        pending = self.schedular.get_num_pending_requests()\n",
    "        running = self.schedular.get_num_running_requests()\n",
    "        total_request = len(self.schedular.requests)\n",
    "        return pending, running, total_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0483e-228f-4939-93ee-356da91cd7e0",
   "metadata": {},
   "source": [
    "## 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ed6a649-1304-4928-9269-227b0330314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)  \n",
    "\n",
    "\n",
    "def listen_request(config, p=0.01):\n",
    "    prompt=[]\n",
    "    prompt_len=0\n",
    "    num = random.randint(1,100)\n",
    "    if num/100.0 < p:\n",
    "        prompt_len = random.randint(config.max_prompt_len//4, config.max_prompt_len)\n",
    "        prompt=torch.randint(low=1, high=config.vocab_size, size=(1, prompt_len))\n",
    "        prompt=prompt[0].tolist()\n",
    "    return prompt, prompt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899de9a0-0698-402a-9e39-1ca1d2aa755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Request Info] pending:1/running:0/total:1\n",
      "[ALLOCATE] request ID0 pages len 1\n",
      "[Request Info] pending:1/running:1/total:2\n",
      "[ALLOCATE] request ID1 pages len 1\n",
      "Running...: * ---------\n",
      "[Request Info] pending:1/running:2/total:3\n",
      "[ALLOCATE] request ID2 pages len 1\n",
      "finished: ID.2, new_len:3\n",
      "[FREE] request ID2 pages, len1\n",
      "[Request Info] pending:1/running:2/total:4\n",
      "[ALLOCATE] request ID3 pages len 1\n",
      "[Request Info] pending:1/running:3/total:5\n",
      "finished: ID.1, new_len:7\n",
      "[FREE] request ID1 pages, len1\n",
      "[ALLOCATE] request ID4 pages len 1\n",
      "Running...: ** --------\n",
      "[Request Info] pending:1/running:3/total:6\n",
      "finished: ID.4, new_len:2\n",
      "[FREE] request ID4 pages, len1\n",
      "[ALLOCATE] request ID5 pages len 1\n",
      "[Request Info] pending:1/running:3/total:7\n",
      "[ALLOCATE] request ID6 pages len 1\n",
      "finished: ID.6, new_len:6\n",
      "[FREE] request ID6 pages, len1\n",
      "[Request Info] pending:1/running:3/total:8\n",
      "[ALLOCATE] request ID7 pages len 1\n",
      "Running...: *** -------\n",
      "[Request Info] pending:1/running:4/total:9\n",
      "[ALLOCATE] request ID8 pages len 1\n",
      "finished: ID.8, new_len:1\n",
      "finished: ID.3, new_len:13\n",
      "[FREE] request ID3 pages, len1\n",
      "[Request Info] pending:1/running:3/total:10\n",
      "[ALLOCATE] request ID9 pages len 1\n",
      "[Request Info] pending:1/running:4/total:11\n",
      "[ALLOCATE] request ID10 pages len 1\n",
      "Running...: **** ------\n",
      "[Request Info] pending:1/running:5/total:12\n",
      "[ALLOCATE] request ID11 pages len 1\n",
      "[Request Info] pending:1/running:6/total:13\n",
      "finished: ID.0, new_len:23\n",
      "[FREE] request ID0 pages, len1\n",
      "[ALLOCATE] request ID12 pages len 1\n",
      "[Request Info] pending:1/running:6/total:14\n",
      "finished: ID.5, new_len:16\n",
      "[FREE] request ID5 pages, len1\n",
      "finished: ID.10, new_len:4\n",
      "[FREE] request ID10 pages, len1\n",
      "[ALLOCATE] request ID13 pages len 1\n",
      "Running...: ***** -----\n",
      "[Request Info] pending:1/running:5/total:15\n",
      "[ALLOCATE] request ID14 pages len 1\n",
      "finished: ID.14, new_len:1\n",
      "[Request Info] pending:1/running:5/total:16\n",
      "[ALLOCATE] request ID15 pages len 1\n",
      "finished: ID.11, new_len:10\n",
      "[FREE] request ID11 pages, len1\n",
      "finished: ID.15, new_len:2\n",
      "[FREE] request ID15 pages, len1\n",
      "[Request Info] pending:1/running:4/total:17\n",
      "finished: ID.9, new_len:13\n",
      "[FREE] request ID9 pages, len1\n",
      "[ALLOCATE] request ID16 pages len 1\n",
      "Running...: ****** ----\n",
      "[Request Info] pending:1/running:4/total:18\n",
      "finished: ID.7, new_len:18\n",
      "[FREE] request ID7 pages, len1\n",
      "finished: ID.12, new_len:12\n",
      "[FREE] request ID12 pages, len1\n",
      "finished: ID.13, new_len:11\n",
      "[FREE] request ID13 pages, len1\n",
      "[ALLOCATE] request ID17 pages len 1\n",
      "[Request Info] pending:1/running:2/total:19\n",
      "finished: ID.17, new_len:2\n",
      "[FREE] request ID17 pages, len1\n",
      "[ALLOCATE] request ID18 pages len 1\n",
      "[Request Info] pending:1/running:2/total:20\n",
      "[ALLOCATE] request ID19 pages len 1\n",
      "Running...: ******* ---\n",
      "[Request Info] pending:1/running:3/total:21\n",
      "[ALLOCATE] request ID20 pages len 1\n",
      "[Request Info] pending:1/running:4/total:22\n",
      "[ALLOCATE] request ID21 pages len 1\n",
      "[Request Info] pending:1/running:5/total:23\n",
      "[ALLOCATE] request ID22 pages len 1\n",
      "Running...: ******** --\n",
      "[Request Info] pending:1/running:6/total:24\n",
      "finished: ID.18, new_len:7\n",
      "[FREE] request ID18 pages, len1\n",
      "[ALLOCATE] request ID23 pages len 1\n",
      "[Request Info] pending:1/running:6/total:25\n",
      "finished: ID.16, new_len:11\n",
      "[FREE] request ID16 pages, len1\n",
      "[ALLOCATE] request ID24 pages len 1\n",
      "finished: ID.24, new_len:2\n",
      "[FREE] request ID24 pages, len1\n",
      "[Request Info] pending:1/running:5/total:26\n",
      "[ALLOCATE] request ID25 pages len 1\n",
      "Running...: ********* -\n",
      "[Request Info] pending:1/running:6/total:27\n",
      "[ALLOCATE] request ID26 pages len 1\n",
      "[Request Info] pending:1/running:7/total:28\n",
      "[ALLOCATE] request ID27 pages len 1\n",
      "[Request Info] pending:1/running:8/total:29\n",
      "[ALLOCATE] request ID28 pages len 1\n",
      "finished: ID.20, new_len:12\n",
      "[FREE] request ID20 pages, len1\n",
      "finished: ID.22, new_len:11\n",
      "[FREE] request ID22 pages, len1\n",
      "Running...: ********** \n",
      "[Request Info] pending:1/running:7/total:30\n",
      "finished: ID.25, new_len:7\n",
      "[FREE] request ID25 pages, len1\n",
      "[ALLOCATE] request ID29 pages len 1\n",
      "finished: ID.29, new_len:1\n",
      "[Request Info] pending:1/running:6/total:31\n",
      "[ALLOCATE] request ID30 pages len 1\n",
      "[Request Info] pending:1/running:7/total:32\n",
      "[ALLOCATE] request ID31 pages len 1\n",
      "finished: ID.26, new_len:12\n",
      "[FREE] request ID26 pages, len1\n",
      "finished: ID.28, new_len:10\n",
      "[FREE] request ID28 pages, len1\n",
      "finished: ID.21, new_len:20\n",
      "[FREE] request ID21 pages, len1\n",
      "finished: ID.30, new_len:8\n",
      "[FREE] request ID30 pages, len1\n",
      "finished: ID.23, new_len:21\n",
      "[FREE] request ID23 pages, len1\n",
      "finished: ID.19, new_len:27\n",
      "[FREE] request ID19 pages, len1\n",
      "finished: ID.27, new_len:17\n",
      "[FREE] request ID27 pages, len1\n",
      "finished: ID.31, new_len:18\n",
      "[FREE] request ID31 pages, len1\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "N = 32\n",
    "count = 0\n",
    "\n",
    "model = ToyModel(config)\n",
    "engine = vLLMPageCacheEngine(model, config)\n",
    "\n",
    "# main \n",
    "while 1:\n",
    "    # 监听进程\n",
    "    if count != N:\n",
    "        prompt, prompt_len = listen_request(config, p=0.5)\n",
    "        if prompt_len != 0:\n",
    "            count += 1\n",
    "            if count % (N//10) == 0:\n",
    "                per = count / (N//10) \n",
    "                print('Running...:','*'*int(per),'-'*(10-int(per)))\n",
    "            generate_len = random.randint(prompt_len, config.max_seq_len)\n",
    "            engine.add_request(prompt, generate_len)\n",
    "            pending, running, total = engine.get_requests_info()\n",
    "            print(f'[Request Info] pending:{pending}/running:{running}/total:{total}')\n",
    "            \n",
    "    # 处理进程   \n",
    "    engine.step()\n",
    "\n",
    "    if not engine.has_pending_work() and count == N:\n",
    "        print('process done')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
